[
  {
    "objectID": "posts/1-2.회귀(회귀모형,손실함수,파이토치를이용한추정).html",
    "href": "posts/1-2.회귀(회귀모형,손실함수,파이토치를이용한추정).html",
    "title": "1-2, 2-1. 회귀(회귀모형, 손실함수, 파이토치를 이용한 추정)",
    "section": "",
    "text": "1. imports\n\nimport torch\nimport matplotlib.pyplot as plt \n\n\nplt.rcParams['figure.figsize'] = (4.5, 3.0)\n\n\n\n2. 회귀모형\n\nA. 아이스 아메리카노 (가짜자료)\n- 카페주인 이상민씨는 온도와 아이스 아메리카노 판매량이 관계가 있다는 것을 확인하기 위해 하래의 100개의 데이터를 모았다.\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\n\n\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\n\n- temp는 평균기온, sales는 아이스 아메리카노 판매량\n- 그래프를 그려보자\n\nplt.plot(temp,sales,'o')\n\n\n\n\n\n\n\n\n- 오늘 평균 기온이 0.5도이면 아이스 아메리카노가 얼마나 팔릴까?\n\n\nB. 자료를 만든 방법\n- 방법1 : \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\n- sort()를 하면 인덱스 항이 생겨서 필요없으므로 _에 저장\n\nx[:5], y[:5]\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n- 방법2: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)\n\n\nX = torch.stack([torch.ones(100),x],axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\nx = X[:,[1]]\n\n\nX[:5,:], y[:5,:]\n\n(tensor([[ 1.0000, -2.4821],\n         [ 1.0000, -2.3621],\n         [ 1.0000, -1.9973],\n         [ 1.0000, -1.6239],\n         [ 1.0000, -1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n- true 와 관측값(observed data) 동시에 시각화\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\nplt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nC. 회귀분석\n- 관측한 자료 \\((x_i,y_i)\\) 이 선형성을 가지고 있을 때 이를 파악하여 새로운 \\(x\\)가 주어졌을 때 \\(\\hat{y}\\)(예측값)을 구할 수 있는 적당한 추세선을 찾는 것\n- 좀 더 정확하게 말하면 \\((x_1,y_1) \\dots (x_n,y_n)\\) 으로\n\\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) 를 최대한 \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)와 비슷하게 찾는 것.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\n- 추세선을 그리는 행위 = \\((w_0,w_1)\\)을 선택하는일\n\n\n\n4. 손실함수\n- \\((\\hat{w}_0,\\hat{w}_1)=(-5,10)\\)을 선택하여 선을 그려보고 적당한지 판단해보자\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\") \nWhat = torch.tensor([[-5.0],[10.0]])\nplt.plot(x,X@What,'--',label=r\"estimated line: $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- 기울기와 절편 모두 너무 다르다\n- \\((\\hat{w}_0,\\hat{w}_1)=(2.5,3.5)\\)을 선택하여 선을 그려보고 적당한지 판단해보자\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\") \nWhat = torch.tensor([[2.5],[3.5]])\nplt.plot(x,X@What,'--',label=r\"estimated line: $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- 기울기가 살짝 다른 듯 하다\n- \\((\\hat{w}_0,\\hat{w}_1)=(2.3,3.5)\\)을 선택하여 선을 그려보고 적당한지 판단해보자\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\") \nWhat = torch.tensor([[2.3],[3.5]])\nplt.plot(x,X@What,'--',label=r\"estimated: $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- \\((\\hat{w}_0,\\hat{w}_1)=(2.5,3.5)\\)를 했을 때와 \\((2.3,3,5)\\) 로 했을 때 중 어떤 것이 더 적당한가?\n\nA. loss 개념\n- (2.5,3.5) 가 더 적당해야할 것 같긴 한데 육안으로 판단 어려움\n- 이를 수식화하기 위해서 : loss의 개념 사용\n\n\\(loss = \\sum_{i=1}^{n}(y_i- \\hat{y}_i)^2 = \\sum_{i=1}^{n}\\big(y_i - (\\hat{w}_0+\\hat{w}_1x_i)\\big)^2\\)\n\n\\(=({\\bf y}-\\hat{\\bf y})^\\top({\\bf y}-\\hat{\\bf y})=({\\bf y}-{\\bf X}\\hat{\\bf W})^\\top({\\bf y}-{\\bf X}\\hat{\\bf W})\\)\n\n\nB. loss의 특징\n\n\\(y_i \\approx \\hat{y}_i\\) 일수록 loss 값이 작음\n\\(y_i \\approx \\hat{y}_i\\) 이 되도록 \\((\\hat{w}_0, \\hat{w}_1)\\)을 잘 찍으면 loss 값이 작음\n주황색 점선이 “적당할수록” loss 값이 작음\n\n\n\nC. loss 사용\n- 방법1 : \\(\\sum_{i=1}^{n}(y_i- \\hat{y}_i)^2\\)\n\nWhat = torch.tensor([[2.5],[3.5]])\nprint(f\"loss: {torch.sum((y - X@What)**2)}\")\n\nWhat = torch.tensor([[2.3],[3.5]])\nprint(f\"loss: {torch.sum((y - X@What)**2)}\")\n\nloss: 55.074012756347656\nloss: 59.3805046081543\n\n\n- 방법2 : \\(({\\bf y}-\\hat{\\bf y})^\\top({\\bf y}-\\hat{\\bf y})\\)\n\nWhat = torch.tensor([[2.5],[3.5]])\nprint(f\"loss: {(y - X@What).T @ (y - X@What)}\")\n\nWhat = torch.tensor([[2.3],[3.5]])\nprint(f\"loss: {(y - X@What).T @ (y - X@What)}\")\n\nloss: tensor([[55.0740]])\nloss: tensor([[59.3805]])\n\n\n\n\n\n5. 파이토치를 이용한 반복추정\n- 추정 전략 : 손실함수 + 경사하강법 * 1단계 : 아무 점선 긋기 * 2단계 : 1단계의 점선보다 loss값이 작은 하나의 직선으로 변경 * 3단계 : 1,2단계 반복\n\nA. 1단계 - 최초 점선\n- What 아무렇게나 설정\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\n\n\n\n\n\n\n\n\n\n\nB. 2단계 - update\n- ‘적당한 정도’ : loss 값이 작을수록 적당함\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat)\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\n- 현재 loss(=8587.6875)를 줄여야함\n\n최종적으로loss를 최소로 하는 \\((\\hat{w}_0,\\hat{w}_1)\\)을 구해야함\n함수의 최대값, 최소값을 컴퓨터로 찾는것 : ‘최적화’\n최적화의 방법 : 경사하강법\n\n- 경사하강법 (1차원)\n\n임의의 점을 찍음\n그 점에서 순간기울기를 구함 (접선) &lt;– 미분\n순간기울기(=미분계수)의 부호와 반대방향으로 점을 이동\n\n\n기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절 \\(\\to\\) \\(\\alpha\\)를 도입\n최종수식 :\\(\\hat{w} \\leftarrow \\hat{w} - \\alpha \\times \\frac{\\partial}{\\partial w}loss(w)\\)\n\n- 경사하강법 (2차원)\n\n\n임의의 점을 찍음\n그 점에서 순간기울기를 구함 (접평면) &lt;– 편미분\n순간기울기(=미분계수)의 부호와 반대방향으로 각각 점을 이동\n\n\n기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절 \\(\\to\\) \\(\\alpha\\)를 도입\n\n- 경사하강법 : loss를 줄이도록 \\(\\hat{W}\\)를 개선하는 방법\n\n수정값 = 원래값 - \\(\\alpha\\) \\(\\times\\) 기울어진 크기(=미분계수)\n\n미분계수와 반대방향으로 이동해야하기 때문에 마이너스 부호 사용\n\n\\(\\alpha\\)는 전체적인 보폭 크기 결정, 클수록 한번에 update에서 움직임이 큼"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DL2025",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 2, 2025\n\n\n1-1. torch(파이토치 기본)\n\n\n이상민 \n\n\n\n\nApr 2, 2025\n\n\n1-2, 2-1. 회귀(회귀모형, 손실함수, 파이토치를 이용한 추정)\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/1-1.torch(파이토치기본).html",
    "href": "posts/1-1.torch(파이토치기본).html",
    "title": "1-1. torch(파이토치 기본)",
    "section": "",
    "text": "1.import\n\nimport torch\n\n\n\n2.기초 지식\n- 선형대수학\n\n벡터와 행렬\n행렬의 곱셉\n트랜스포즈\n\n- 기초통계학(수리통계)\n\n정규분포, 이항분포\n모수, 추정\n\\(X_i \\overset{i.i.d.}{\\sim} N(0,1)\\)\n\n- 회귀분석\n\n독립변수(\\(y\\)), 설명변수(\\(X\\))\n\\({\\boldsymbol y} = {\\bf X}{\\boldsymbol \\beta} + {\\boldsymbol \\epsilon}\\)\n\n- 파이썬\n\n파이썬 기본문법\n넘파이, 판다스\n전반적인 클래스 지식 (__init__, self, …)\n상속\n\n\n\n3. torch\n\nA.벡터\n\ntorch.tensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n- 벡터끼리 덧셈\n\ntorch.tensor([1,2,3]) + torch.tensor([3,3,3])\n\ntensor([4, 5, 6])\n\n\n- 브로드캐스팅 가능 -&gt; 위에와 똑같은 기능\n\ntorch.tensor([1,2,3])+2\n\ntensor([3, 4, 5])\n\n\n\ntorch.tensor([1,2,3])+torch.tensor([2])\n\ntensor([3, 4, 5])\n\n\n\ntorch.tensor([1,2,3])+torch.tensor(2)\n\ntensor([3, 4, 5])\n\n\n\n\n\nB. 벡터와 매트릭스\n- 3x2 matrix\n\ntorch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- 3X1 matrix 는 3X1 열벡터(column vector)와 같음\n\ntorch.tensor([[1],[2],[3]]) \n\ntensor([[1],\n        [2],\n        [3]])\n\n\n- 1X2 matrix 는 1X2 행벡터(row vector)와 같음\n\ntorch.tensor([[1,2]]) \n\ntensor([[1, 2]])\n\n\n\nc. matrix 덧셈\n- 브로드캐스팅(숫자하나)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - 1\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\n- 아래와 같은 의미임\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - torch.tensor([[1,1],[1,1],[1,1]])\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\n- 브로드캐스팅(열)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-3],[-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\n- 아래와 같은 의미임\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-1],[-3,-3],[-5,-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\n- 브로드캐스팅(행)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n- 아래와 같은 의미임\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n잘못된 브로드캐스팅\n- 열로 브로드캐스팅 하려면 3X1 행렬이어야하지만 여기는 1X3 행렬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\n- 행으로 브로드캐스팅 하려면 1X2 행렬이어야하지만 여기는 2X1 행렬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\n그냥 벡터를 넣으면 이상하게 행으로만 브로드캐스팅 됨\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-2])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n- 열로는 안됨..\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\n\n\nD. 행렬곱\n정상적 행렬곱\n- (3X2) @ (2X1) = (3X1)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1],[2]])\n\ntensor([[ 5],\n        [11],\n        [17]])\n\n\n- (1X3) @ (3X2) = (1X2)\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[22, 28]])\n\n\n잘못된 행렬곱\n- (3X2) @ (1X2) = (???)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)\n\n\n\n- (3X1) @ (3X2) = (???)\n\ntorch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 torch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x2)\n\n\n\n이상하게 되는 것\n- (3X2) @ 행벡터(1X2)-&gt;(2X1)행렬로 바꿔주는듯 = (3X1) 행렬이 아닌 (1X3)행벡터로 나옴\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([1,2])\n\ntensor([ 5, 11, 17])\n\n\n- 행벡터(1X3) @ (3X2) = (1X2)\n\ntorch.tensor([1,2,3]) @ torch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([22, 28])\n\n\n\n\nE. Transpose\n- 정방행렬 전치\n\ntorch.tensor([[1,2],[3,4]]).T \n\ntensor([[1, 3],\n        [2, 4]])\n\n\n- (NX1) 행렬 전치\n\ntorch.tensor([[1],[3]]).T \n\ntensor([[1, 3]])\n\n\n- (1XN) 행렬 전치\n\ntorch.tensor([[1,2]]).T \n\ntensor([[1],\n        [2]])\n\n\n\n\nF. reshape\n- 일반적인 사용\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,3)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n- Transpose와는 다르게 순서대로 reshape 해줌\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6,1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n- 차원 줄이기도 가능\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n- -1로 설정한 부분은 자동으로 지정됨\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,-1)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n- -1만 넣으면 행벡터로 만들어버림\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n\ntorch.tensor([[[1,2],[2,30]],[[1,2],[3,3]]]).reshape(-1)\n\ntensor([ 1,  2,  2, 30,  1,  2,  3,  3])\n\n\n\n\nG. concat, stack (★★★)\n- concat\n\naxis=0 인 경우 0번째 차원을 기준으로 합쳐짐\naxis=1 인 경우 1번째 차원을 기준으로 합쳐짐\n\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=0)\n\ntensor([[1],\n        [3],\n        [5],\n        [2],\n        [4],\n        [6]])\n\n\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- stack\n\naxis=0 : 0번째 차원을 추가\naxis=1 : 1번째 차원을 추가\n\n\na = torch.tensor([1,3,5])\nb = torch.tensor([2,4,6])\ntorch.stack([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  }
]