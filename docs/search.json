[
  {
    "objectID": "posts/1-2.회귀(회귀모형,손실함수,파이토치를이용한추정).html",
    "href": "posts/1-2.회귀(회귀모형,손실함수,파이토치를이용한추정).html",
    "title": "1-2, 2-1. 회귀(회귀모형, 손실함수, 파이토치를 이용한 추정)",
    "section": "",
    "text": "1. imports\n\nimport torch\nimport matplotlib.pyplot as plt \n\n\nplt.rcParams['figure.figsize'] = (4.5, 3.0)\n\n\n\n2. 회귀모형\n\nA. 아이스 아메리카노 (가짜자료)\n- 카페주인 이상민씨는 온도와 아이스 아메리카노 판매량이 관계가 있다는 것을 확인하기 위해 하래의 100개의 데이터를 모았다.\n\ntemp = [-2.4821, -2.3621, -1.9973, -1.6239, -1.4792, -1.4635, -1.4509, -1.4435,\n        -1.3722, -1.3079, -1.1904, -1.1092, -1.1054, -1.0875, -0.9469, -0.9319,\n        -0.8643, -0.7858, -0.7549, -0.7421, -0.6948, -0.6103, -0.5830, -0.5621,\n        -0.5506, -0.5058, -0.4806, -0.4738, -0.4710, -0.4676, -0.3874, -0.3719,\n        -0.3688, -0.3159, -0.2775, -0.2772, -0.2734, -0.2721, -0.2668, -0.2155,\n        -0.2000, -0.1816, -0.1708, -0.1565, -0.1448, -0.1361, -0.1057, -0.0603,\n        -0.0559, -0.0214,  0.0655,  0.0684,  0.1195,  0.1420,  0.1521,  0.1568,\n         0.2646,  0.2656,  0.3157,  0.3220,  0.3461,  0.3984,  0.4190,  0.5443,\n         0.5579,  0.5913,  0.6148,  0.6469,  0.6469,  0.6523,  0.6674,  0.7059,\n         0.7141,  0.7822,  0.8154,  0.8668,  0.9291,  0.9804,  0.9853,  0.9941,\n         1.0376,  1.0393,  1.0697,  1.1024,  1.1126,  1.1532,  1.2289,  1.3403,\n         1.3494,  1.4279,  1.4994,  1.5031,  1.5437,  1.6789,  2.0832,  2.2444,\n         2.3935,  2.6056,  2.6057,  2.6632]\n\n\nsales= [-8.5420, -6.5767, -5.9496, -4.4794, -4.2516, -3.1326, -4.0239, -4.1862,\n        -3.3403, -2.2027, -2.0262, -2.5619, -1.3353, -2.0466, -0.4664, -1.3513,\n        -1.6472, -0.1089, -0.3071, -0.6299, -0.0438,  0.4163,  0.4166, -0.0943,\n         0.2662,  0.4591,  0.8905,  0.8998,  0.6314,  1.3845,  0.8085,  1.2594,\n         1.1211,  1.9232,  1.0619,  1.3552,  2.1161,  1.1437,  1.6245,  1.7639,\n         1.6022,  1.7465,  0.9830,  1.7824,  2.1116,  2.8621,  2.1165,  1.5226,\n         2.5572,  2.8361,  3.3956,  2.0679,  2.8140,  3.4852,  3.6059,  2.5966,\n         2.8854,  3.9173,  3.6527,  4.1029,  4.3125,  3.4026,  3.2180,  4.5686,\n         4.3772,  4.3075,  4.4895,  4.4827,  5.3170,  5.4987,  5.4632,  6.0328,\n         5.2842,  5.0539,  5.4538,  6.0337,  5.7250,  5.7587,  6.2020,  6.5992,\n         6.4621,  6.5140,  6.6846,  7.3497,  8.0909,  7.0794,  6.8667,  7.4229,\n         7.2544,  7.1967,  9.5006,  9.0339,  7.4887,  9.0759, 11.0946, 10.3260,\n        12.2665, 13.0983, 12.5468, 13.8340]\n\n- temp는 평균기온, sales는 아이스 아메리카노 판매량\n- 그래프를 그려보자\n\nplt.plot(temp,sales,'o')\n\n\n\n\n\n\n\n\n- 오늘 평균 기온이 0.5도이면 아이스 아메리카노가 얼마나 팔릴까?\n\n\nB. 자료를 만든 방법\n- 방법1 : \\(y_i= w_0+w_1 x_i +\\epsilon_i = 2.5 + 4x_i +\\epsilon_i, \\quad i=1,2,\\dots,n\\)\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\ny = x * 4 + 2.5 + eps\n\n- sort()를 하면 인덱스 항이 생겨서 필요없으므로 _에 저장\n\nx[:5], y[:5]\n\n(tensor([-2.4821, -2.3621, -1.9973, -1.6239, -1.4792]),\n tensor([-8.5420, -6.5767, -5.9496, -4.4794, -4.2516]))\n\n\n- 방법2: \\({\\bf y}={\\bf X}{\\bf W} +\\boldsymbol{\\epsilon}\\)\n\n\\({\\bf y}=\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\dots \\\\ y_n\\end{bmatrix}, \\quad {\\bf X}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\dots \\\\ 1 & x_n\\end{bmatrix}, \\quad {\\bf W}=\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}, \\quad \\boldsymbol{\\epsilon}= \\begin{bmatrix} \\epsilon_1 \\\\ \\dots \\\\ \\epsilon_n\\end{bmatrix}\\)\n\n\nX = torch.stack([torch.ones(100),x],axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\nx = X[:,[1]]\n\n\nX[:5,:], y[:5,:]\n\n(tensor([[ 1.0000, -2.4821],\n         [ 1.0000, -2.3621],\n         [ 1.0000, -1.9973],\n         [ 1.0000, -1.6239],\n         [ 1.0000, -1.4792]]),\n tensor([[-8.5420],\n         [-6.5767],\n         [-5.9496],\n         [-4.4794],\n         [-4.2516]]))\n\n\n- true 와 관측값(observed data) 동시에 시각화\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\")\nplt.plot(x,2.5+4*x,'--',label=r\"true: $(x_i, 4x_i+2.5)$ // $y=4x+2.5$ \")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nC. 회귀분석\n- 관측한 자료 \\((x_i,y_i)\\) 이 선형성을 가지고 있을 때 이를 파악하여 새로운 \\(x\\)가 주어졌을 때 \\(\\hat{y}\\)(예측값)을 구할 수 있는 적당한 추세선을 찾는 것\n- 좀 더 정확하게 말하면 \\((x_1,y_1) \\dots (x_n,y_n)\\) 으로\n\\(\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\) 를 최대한 \\(\\begin{bmatrix} 2.5 \\\\ 4 \\end{bmatrix}\\)와 비슷하게 찾는 것.\n\ngiven data : \\(\\big\\{(x_i,y_i) \\big\\}_{i=1}^{n}\\)\nparameter: \\({\\bf W}=\\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}\\)\nestimated parameter: \\({\\bf \\hat{W}}=\\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix}\\)\n\n- 추세선을 그리는 행위 = \\((w_0,w_1)\\)을 선택하는일\n\n\n\n4. 손실함수\n- \\((\\hat{w}_0,\\hat{w}_1)=(-5,10)\\)을 선택하여 선을 그려보고 적당한지 판단해보자\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\") \nWhat = torch.tensor([[-5.0],[10.0]])\nplt.plot(x,X@What,'--',label=r\"estimated line: $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- 기울기와 절편 모두 너무 다르다\n- \\((\\hat{w}_0,\\hat{w}_1)=(2.5,3.5)\\)을 선택하여 선을 그려보고 적당한지 판단해보자\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\") \nWhat = torch.tensor([[2.5],[3.5]])\nplt.plot(x,X@What,'--',label=r\"estimated line: $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- 기울기가 살짝 다른 듯 하다\n- \\((\\hat{w}_0,\\hat{w}_1)=(2.3,3.5)\\)을 선택하여 선을 그려보고 적당한지 판단해보자\n\nplt.plot(x,y,'o',label=r\"observed data: $(x_i,y_i)$\") \nWhat = torch.tensor([[2.3],[3.5]])\nplt.plot(x,X@What,'--',label=r\"estimated: $(x_i,\\hat{y}_i)$\")\nplt.legend()\n\n\n\n\n\n\n\n\n- \\((\\hat{w}_0,\\hat{w}_1)=(2.5,3.5)\\)를 했을 때와 \\((2.3,3,5)\\) 로 했을 때 중 어떤 것이 더 적당한가?\n\nA. loss 개념\n- (2.5,3.5) 가 더 적당해야할 것 같긴 한데 육안으로 판단 어려움\n- 이를 수식화하기 위해서 : loss의 개념 사용\n\n\\(loss = \\sum_{i=1}^{n}(y_i- \\hat{y}_i)^2 = \\sum_{i=1}^{n}\\big(y_i - (\\hat{w}_0+\\hat{w}_1x_i)\\big)^2\\)\n\n\\(=({\\bf y}-\\hat{\\bf y})^\\top({\\bf y}-\\hat{\\bf y})=({\\bf y}-{\\bf X}\\hat{\\bf W})^\\top({\\bf y}-{\\bf X}\\hat{\\bf W})\\)\n\n\nB. loss의 특징\n\n\\(y_i \\approx \\hat{y}_i\\) 일수록 loss 값이 작음\n\\(y_i \\approx \\hat{y}_i\\) 이 되도록 \\((\\hat{w}_0, \\hat{w}_1)\\)을 잘 찍으면 loss 값이 작음\n주황색 점선이 “적당할수록” loss 값이 작음\n\n\n\nC. loss 사용\n- 방법1 : \\(\\sum_{i=1}^{n}(y_i- \\hat{y}_i)^2\\)\n\nWhat = torch.tensor([[2.5],[3.5]])\nprint(f\"loss: {torch.sum((y - X@What)**2)}\")\n\nWhat = torch.tensor([[2.3],[3.5]])\nprint(f\"loss: {torch.sum((y - X@What)**2)}\")\n\nloss: 55.074012756347656\nloss: 59.3805046081543\n\n\n- 방법2 : \\(({\\bf y}-\\hat{\\bf y})^\\top({\\bf y}-\\hat{\\bf y})\\)\n\nWhat = torch.tensor([[2.5],[3.5]])\nprint(f\"loss: {(y - X@What).T @ (y - X@What)}\")\n\nWhat = torch.tensor([[2.3],[3.5]])\nprint(f\"loss: {(y - X@What).T @ (y - X@What)}\")\n\nloss: tensor([[55.0740]])\nloss: tensor([[59.3805]])\n\n\n\n\n\n5. 파이토치를 이용한 반복추정\n- 추정 전략 : 손실함수 + 경사하강법 * 1단계 : 아무 점선 긋기 * 2단계 : 1단계의 점선보다 loss값이 작은 하나의 직선으로 변경 * 3단계 : 1,2단계 반복\n\nA. 1단계 - 최초 점선\n- What 아무렇게나 설정\n\nWhat = torch.tensor([[-5.0],[10.0]])\nWhat\n\ntensor([[-5.],\n        [10.]])\n\n\n\nyhat = X@What\n\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat.data,'--')\n\n\n\n\n\n\n\n\n\n\nB. 2단계 - update\n- ‘적당한 정도’ : loss 값이 작을수록 적당함\n\nplt.plot(x,y,'o')\nplt.plot(x,yhat)\n\n\n\n\n\n\n\n\n\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875)\n\n\n- 현재 loss(=8587.6875)를 줄여야함\n\n최종적으로loss를 최소로 하는 \\((\\hat{w}_0,\\hat{w}_1)\\)을 구해야함\n함수의 최대값, 최소값을 컴퓨터로 찾는것 : ‘최적화’\n최적화의 방법 : 경사하강법\n\n- 경사하강법 (1차원)\n\n임의의 점을 찍음\n그 점에서 순간기울기를 구함 (접선) &lt;– 미분\n순간기울기(=미분계수)의 부호와 반대방향으로 점을 이동\n\n\n기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 조절 \\(\\to\\) \\(\\alpha\\)를 도입\n최종수식 :\\(\\hat{w} \\leftarrow \\hat{w} - \\alpha \\times \\frac{\\partial}{\\partial w}loss(w)\\)\n\n- 경사하강법 (2차원)\n\n\n임의의 점을 찍음\n그 점에서 순간기울기를 구함 (접평면) &lt;– 편미분\n순간기울기(=미분계수)의 부호와 반대방향으로 각각 점을 이동\n\n\n기울기의 절대값 크기와 비례하여 보폭(=움직이는 정도)을 각각 조절 \\(\\to\\) \\(\\alpha\\)를 도입\n\n- 경사하강법 : loss를 줄이도록 \\(\\hat{W}\\)를 개선하는 방법\n\n수정값 = 원래값 - \\(\\alpha\\) \\(\\times\\) 기울어진 크기(=미분계수)\n\n미분계수와 반대방향으로 이동해야하기 때문에 마이너스 부호 사용\n\n\\(\\alpha\\)는 전체적인 보폭 크기 결정, 클수록 한번에 update에서 움직임이 큼\n\n- 우리가 구하고 싶은 것\n\n\\(\\hat{W}^{LSE}=\\underset{\\hat{W}}argmin ~ loss(\\hat{W})\\)\n\n- 요약\n\nx,X,W,y // X = [1 x], W = [w0, w1] (회귀분석에서는 W=β)\n회귀모형: y=X@W+ϵ = X@β+ϵ\ntrue: E(y)=X@W\nobserved: (x,y)\nestimated W = What = [w0hat, w1hat]’ &lt;– 아무값이나넣음\nestimated y = yhat = X@What = X@β̂\nloss = yhat이랑 y랑 얼마나 비슷한지 = sum((y-yhat)^2)\n(x,y) 보고 최적의 선분을 그리는것 = loss를 가장 작게 만드는 What = [w0hat, w1hat] 를 찾는것\n전략\n\n\n아무 What나 찍는다\n\n\n그거보다 더 나은 What을 찾는다.\n\n\n1-2를 반복한다.\n\n\n전략2가 어려운데, 이를 수행하는 방법이 경사하강법\n경사하강법 알고리즘: 더나은What = 원래What - \\(\\alpha\\)*미분값\n수식 \\[\\hat{\\bf W} \\leftarrow \\hat{\\bf W} - \\alpha \\times \\left.\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})\\right|_{{\\bf W}=\\hat{\\bf W}}\\]\n\n- 미분값 계산법 1) \\(\\to\\) 정확하지도 않고 번거로운 방법..\n\ndef l(w0,w1):\n    yhat = w0 + w1*x\n    return torch.sum((y-yhat)**2)\n\n\nl(-5,10)\n\ntensor(8587.6875)\n\n\n\nh=0.001\nprint((l(-5+h,10) - l(-5,10))/h)\nprint((l(-5,10+h) - l(-5,10))/h)\n\ntensor(-1341.7968)\ntensor(1190.4297)\n\n\n\nnew = What - 0.001 * torch.tensor([[-1341.7968],[1190.4297]])\nnew\n\ntensor([[-3.6582],\n        [ 8.8096]])\n\n\n\nplt.plot(x,y,'o')\nplt.plot(x,X@What,'-') # 원래What: 주황색\nplt.plot(x,X@new,'-') # 더나은What: 초록색\n\n\n\n\n\n\n\n\n- 수식\n\n편미분\n\\(\\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\approx \\frac{loss(w_0+h,w_1)-loss(w_0,w_1)}{h}\\)\n\\(\\frac{\\partial}{\\partial w_1}loss(w_0,w_1) \\approx \\frac{loss(w_0,w_1+h)-loss(w_0,w_1)}{h}\\)\n편미분 값을 이용\n\\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W}):= \\begin{bmatrix} \\frac{\\partial}{\\partial w_0} \\\\ \\frac{\\partial}{\\partial w_1}\\end{bmatrix}loss({\\bf W}) =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss({\\bf W}) \\\\ \\frac{\\partial}{\\partial w_1}loss({\\bf W})\\end{bmatrix}  =  \\begin{bmatrix} \\frac{\\partial}{\\partial w_0}loss(w_0,w_1) \\\\ \\frac{\\partial}{\\partial w_1}loss(w_0,w_1)\\end{bmatrix}\\]\n\n- 미분값 계산법 2) \\(\\to\\) 이것도 어려움…\n\nloss = (y - XWhat)'(y -  XWhat)\n= (y'- What'X')(y - XWhat)\n= y'y - y'XWhat - What'X'y + What'X'XWhat\nloss를 What으로 미분\nloss' = -X'y - X'y + 2X'XWhat \\[\\frac{\\partial}{\\partial {\\bf W}}loss({\\bf W})= -2{\\bf X}^\\top {\\bf y} + 2{\\bf X}^\\top {\\bf X}{\\bf W}\\]\n\n\n-2*X.T@y + 2*X.T@X@What\n\ntensor([[-1342.2524],\n        [ 1188.9302]])\n\n\n- 미분값 계산법 3) (★)\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nWhat\n\ntensor([[-5.],\n        [10.]], requires_grad=True)\n\n\n\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\nloss\n\ntensor(8587.6875, grad_fn=&lt;SumBackward0&gt;)\n\n\n- loss를 꼬리표의 근원인 What으로 미분\n\nloss.backward() \n\n- What 에 미분값이 저장\n\nWhat.grad\n\ntensor([[-1342.2524],\n        [ 1188.9305]])\n\n\n- 미분 전\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True)\nyhat = X@What\nloss = torch.sum((y-yhat)**2)\n\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n None)\n\n\n\nloss.backward()\n\n- 미분 후\n\nWhat.data, What.grad\n\n(tensor([[-5.],\n         [10.]]),\n tensor([[-1342.2524],\n         [ 1188.9305]]))\n\n\n- 1회 업데이트 과정\n\nalpha=0.001\nprint(f\"{What.data} -- 수정전\")\nprint(f\"{-alpha*What.grad} -- 수정하는폭\")\nprint(f\"{What.data-alpha*What.grad} -- 수정후\")\nprint(f\"{torch.tensor([[2.5],[4]])} -- 참값\")\n\ntensor([[-5.],\n        [10.]]) -- 수정전\ntensor([[ 1.3423],\n        [-1.1889]]) -- 수정하는폭\ntensor([[-3.6577],\n        [ 8.8111]]) -- 수정후\ntensor([[2.5000],\n        [4.0000]]) -- 참값\n\n\n\nWbefore = What.data\nWafter = What.data - alpha * What.grad\nplt.plot(x,y,'o',label=r'observed data')\nplt.plot(x,X@Wbefore,'--', label=r\"$\\hat{\\bf y}_{before}={\\bf X}@\\hat{\\bf W}_{before}$\")\nplt.plot(x,X@Wafter,'--', label=r\"$\\hat{\\bf y}_{after}={\\bf X}@\\hat{\\bf W}_{after}$\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nC. 3단계 - iteration\n- What.grad = None을 꼭 해줘야함\n\nloss.backward() 의 역할\n\nWhat.grad \\(\\leftarrow\\) What.grad + What에서의미분값\n\n\n\nWhat = torch.tensor([[-5.0],[10.0]],requires_grad=True) # 최초의 직선을 만드는 값\nfor epoc in range(30):\n    yhat = X@What \n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - 0.001 * What.grad\n    What.grad = None \n\n\nplt.plot(x,y,'o',label=r\"observed: $(x_i,y_i)$\")\nplt.plot(x,X@What.data,'--o', label=r\"estimated: $(x_i,\\hat{y}_i)$ -- after 30 iterations (=epochs)\", alpha=0.4 )\nplt.legend()"
  },
  {
    "objectID": "posts/1-1.torch(파이토치기본).html",
    "href": "posts/1-1.torch(파이토치기본).html",
    "title": "1-1. torch(파이토치 기본)",
    "section": "",
    "text": "1.import\n\nimport torch\n\n\n\n2.기초 지식\n- 선형대수학\n\n벡터와 행렬\n행렬의 곱셉\n트랜스포즈\n\n- 기초통계학(수리통계)\n\n정규분포, 이항분포\n모수, 추정\n\\(X_i \\overset{i.i.d.}{\\sim} N(0,1)\\)\n\n- 회귀분석\n\n독립변수(\\(y\\)), 설명변수(\\(X\\))\n\\({\\boldsymbol y} = {\\bf X}{\\boldsymbol \\beta} + {\\boldsymbol \\epsilon}\\)\n\n- 파이썬\n\n파이썬 기본문법\n넘파이, 판다스\n전반적인 클래스 지식 (__init__, self, …)\n상속\n\n\n\n3. torch\n\nA.벡터\n\ntorch.tensor([1,2,3])\n\ntensor([1, 2, 3])\n\n\n- 벡터끼리 덧셈\n\ntorch.tensor([1,2,3]) + torch.tensor([3,3,3])\n\ntensor([4, 5, 6])\n\n\n- 브로드캐스팅 가능 -&gt; 위에와 똑같은 기능\n\ntorch.tensor([1,2,3])+2\n\ntensor([3, 4, 5])\n\n\n\ntorch.tensor([1,2,3])+torch.tensor([2])\n\ntensor([3, 4, 5])\n\n\n\ntorch.tensor([1,2,3])+torch.tensor(2)\n\ntensor([3, 4, 5])\n\n\n\n\n\nB. 벡터와 매트릭스\n- 3x2 matrix\n\ntorch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- 3X1 matrix 는 3X1 열벡터(column vector)와 같음\n\ntorch.tensor([[1],[2],[3]]) \n\ntensor([[1],\n        [2],\n        [3]])\n\n\n- 1X2 matrix 는 1X2 행벡터(row vector)와 같음\n\ntorch.tensor([[1,2]]) \n\ntensor([[1, 2]])\n\n\n\nc. matrix 덧셈\n- 브로드캐스팅(숫자하나)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - 1\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\n- 아래와 같은 의미임\n\ntorch.tensor([[1,2],[3,4],[5,6]]) - torch.tensor([[1,1],[1,1],[1,1]])\n\ntensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n\n\n- 브로드캐스팅(열)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-3],[-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\n- 아래와 같은 의미임\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-1],[-3,-3],[-5,-5]])\n\ntensor([[0, 1],\n        [0, 1],\n        [0, 1]])\n\n\n- 브로드캐스팅(행)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n- 아래와 같은 의미임\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-2],[-1,-2],[-1,-2]])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n잘못된 브로드캐스팅\n- 열로 브로드캐스팅 하려면 3X1 행렬이어야하지만 여기는 1X3 행렬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1,-3,-5]])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\n- 행으로 브로드캐스팅 하려면 1X2 행렬이어야하지만 여기는 2X1 행렬\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([[-1],[-2]])\n\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0\n\n\n\n그냥 벡터를 넣으면 이상하게 행으로만 브로드캐스팅 됨\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-2])\n\ntensor([[0, 0],\n        [2, 2],\n        [4, 4]])\n\n\n- 열로는 안됨..\n\ntorch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) + torch.tensor([-1,-3,-5])\n\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n\n\n\n\n\nD. 행렬곱\n정상적 행렬곱\n- (3X2) @ (2X1) = (3X1)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1],[2]])\n\ntensor([[ 5],\n        [11],\n        [17]])\n\n\n- (1X3) @ (3X2) = (1X2)\n\ntorch.tensor([[1,2,3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\ntensor([[22, 28]])\n\n\n잘못된 행렬곱\n- (3X2) @ (1X2) = (???)\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 torch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([[1,2]])\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 1x2)\n\n\n\n- (3X1) @ (3X2) = (???)\n\ntorch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 torch.tensor([[1],[2],[3]]) @ torch.tensor([[1,2],[3,4],[5,6]]) \n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x2)\n\n\n\n이상하게 되는 것\n- (3X2) @ 행벡터(1X2)-&gt;(2X1)행렬로 바꿔주는듯 = (3X1) 행렬이 아닌 (1X3)행벡터로 나옴\n\ntorch.tensor([[1,2],[3,4],[5,6]]) @ torch.tensor([1,2])\n\ntensor([ 5, 11, 17])\n\n\n- 행벡터(1X3) @ (3X2) = (1X2)\n\ntorch.tensor([1,2,3]) @ torch.tensor([[1,2],[3,4],[5,6]])\n\ntensor([22, 28])\n\n\n\n\nE. Transpose\n- 정방행렬 전치\n\ntorch.tensor([[1,2],[3,4]]).T \n\ntensor([[1, 3],\n        [2, 4]])\n\n\n- (NX1) 행렬 전치\n\ntorch.tensor([[1],[3]]).T \n\ntensor([[1, 3]])\n\n\n- (1XN) 행렬 전치\n\ntorch.tensor([[1,2]]).T \n\ntensor([[1],\n        [2]])\n\n\n\n\nF. reshape\n- 일반적인 사용\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,3)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n- Transpose와는 다르게 순서대로 reshape 해줌\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6,1)\n\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n- 차원 줄이기도 가능\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(6)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n- -1로 설정한 부분은 자동으로 지정됨\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(2,-1)\n\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1,6)\n\ntensor([[1, 2, 3, 4, 5, 6]])\n\n\n- -1만 넣으면 행벡터로 만들어버림\n\ntorch.tensor([[1,2],[3,4],[5,6]]).reshape(-1)\n\ntensor([1, 2, 3, 4, 5, 6])\n\n\n\ntorch.tensor([[[1,2],[2,30]],[[1,2],[3,3]]]).reshape(-1)\n\ntensor([ 1,  2,  2, 30,  1,  2,  3,  3])\n\n\n\n\nG. concat, stack (★★★)\n- concat\n\naxis=0 인 경우 0번째 차원을 기준으로 합쳐짐\naxis=1 인 경우 1번째 차원을 기준으로 합쳐짐\n\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=0)\n\ntensor([[1],\n        [3],\n        [5],\n        [2],\n        [4],\n        [6]])\n\n\n\na = torch.tensor([[1],[3],[5]])\nb = torch.tensor([[2],[4],[6]])\ntorch.concat([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\n\n\n- stack\n\naxis=0 : 0번째 차원을 추가\naxis=1 : 1번째 차원을 추가\n\n\na = torch.tensor([1,3,5])\nb = torch.tensor([2,4,6])\ntorch.stack([a,b],axis=1)\n\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DL2025",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 27, 2025\n\n\n2-2. 회귀(파라메터 학습과정, MSE, 파이토치식 코딩패턴1)\n\n\n이상민 \n\n\n\n\nMar 26, 2025\n\n\n1-2, 2-1. 회귀(회귀모형, 손실함수, 파이토치를 이용한 추정)\n\n\n이상민 \n\n\n\n\nMar 25, 2025\n\n\n1-1. torch(파이토치 기본)\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2-2.회귀(파라메터학습과정,MSE,파이토치식코딩패턴).html",
    "href": "posts/2-2.회귀(파라메터학습과정,MSE,파이토치식코딩패턴).html",
    "title": "2-2. 회귀(파라메터 학습과정, MSE, 파이토치식 코딩패턴1)",
    "section": "",
    "text": "1. imports\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt \n\n\nplt.rcParams['figure.figsize'] = (4.5, 3.0)\n\n\n\n2. 파라메터 학습과정\n\ntorch.manual_seed(43052)\nx,_ = torch.randn(100).sort()\neps = torch.randn(100)*0.5\nX = torch.stack([torch.ones(100),x],axis=1)\nW = torch.tensor([[2.5],[4.0]])\ny = X@W + eps.reshape(100,1)\nx = X[:,[1]]\n\n\nA. 학습과정 print\n\nWhat = torch.tensor([[-5.0],[10.0]], requires_grad=True)\nalpha = 0.001\nprint(f\"시작값 = {What.data.reshape(-1)}\")\nfor epoc in range(30):\n    yhat = X @ What\n    loss = torch.sum((y-yhat)**2)\n    loss.backward()\n    What.data = What.data - alpha * What.grad\n    print(f'loss = {loss:.2f} \\n업데이트폭 = {-alpha * What.grad.reshape(-1)} \\n업데이트결과: {What.data.reshape(-1)}')\n    What.grad = None\n\n시작값 = tensor([-5., 10.])\nloss = 8587.69 \n업데이트폭 = tensor([ 1.3423, -1.1889]) \n업데이트결과: tensor([-3.6577,  8.8111])\nloss = 5675.21 \n업데이트폭 = tensor([ 1.1029, -0.9499]) \n업데이트결과: tensor([-2.5548,  7.8612])\nloss = 3755.64 \n업데이트폭 = tensor([ 0.9056, -0.7596]) \n업데이트결과: tensor([-1.6492,  7.1016])\nloss = 2489.58 \n업데이트폭 = tensor([ 0.7431, -0.6081]) \n업데이트결과: tensor([-0.9061,  6.4935])\nloss = 1654.04 \n업데이트폭 = tensor([ 0.6094, -0.4872]) \n업데이트결과: tensor([-0.2967,  6.0063])\nloss = 1102.32 \n업데이트폭 = tensor([ 0.4995, -0.3907]) \n업데이트결과: tensor([0.2028, 5.6156])\nloss = 737.84 \n업데이트폭 = tensor([ 0.4091, -0.3136]) \n업데이트결과: tensor([0.6119, 5.3020])\nloss = 496.97 \n업데이트폭 = tensor([ 0.3350, -0.2519]) \n업데이트결과: tensor([0.9469, 5.0501])\nloss = 337.71 \n업데이트폭 = tensor([ 0.2742, -0.2025]) \n업데이트결과: tensor([1.2211, 4.8477])\nloss = 232.40 \n업데이트폭 = tensor([ 0.2243, -0.1629]) \n업데이트결과: tensor([1.4454, 4.6848])\nloss = 162.73 \n업데이트폭 = tensor([ 0.1834, -0.1311]) \n업데이트결과: tensor([1.6288, 4.5537])\nloss = 116.63 \n업데이트폭 = tensor([ 0.1500, -0.1056]) \n업데이트결과: tensor([1.7787, 4.4480])\nloss = 86.13 \n업데이트폭 = tensor([ 0.1226, -0.0851]) \n업데이트결과: tensor([1.9013, 4.3629])\nloss = 65.93 \n업데이트폭 = tensor([ 0.1001, -0.0687]) \n업데이트결과: tensor([2.0014, 4.2942])\nloss = 52.57 \n업데이트폭 = tensor([ 0.0818, -0.0554]) \n업데이트결과: tensor([2.0832, 4.2388])\nloss = 43.72 \n업데이트폭 = tensor([ 0.0668, -0.0447]) \n업데이트결과: tensor([2.1500, 4.1941])\nloss = 37.86 \n업데이트폭 = tensor([ 0.0545, -0.0361]) \n업데이트결과: tensor([2.2045, 4.1579])\nloss = 33.97 \n업데이트폭 = tensor([ 0.0445, -0.0292]) \n업데이트결과: tensor([2.2490, 4.1287])\nloss = 31.40 \n업데이트폭 = tensor([ 0.0363, -0.0236]) \n업데이트결과: tensor([2.2853, 4.1051])\nloss = 29.70 \n업데이트폭 = tensor([ 0.0296, -0.0191]) \n업데이트결과: tensor([2.3150, 4.0860])\nloss = 28.57 \n업데이트폭 = tensor([ 0.0242, -0.0155]) \n업데이트결과: tensor([2.3392, 4.0705])\nloss = 27.83 \n업데이트폭 = tensor([ 0.0197, -0.0125]) \n업데이트결과: tensor([2.3589, 4.0580])\nloss = 27.33 \n업데이트폭 = tensor([ 0.0161, -0.0101]) \n업데이트결과: tensor([2.3750, 4.0479])\nloss = 27.00 \n업데이트폭 = tensor([ 0.0131, -0.0082]) \n업데이트결과: tensor([2.3881, 4.0396])\nloss = 26.79 \n업데이트폭 = tensor([ 0.0107, -0.0067]) \n업데이트결과: tensor([2.3988, 4.0330])\nloss = 26.64 \n업데이트폭 = tensor([ 0.0087, -0.0054]) \n업데이트결과: tensor([2.4075, 4.0276])\nloss = 26.55 \n업데이트폭 = tensor([ 0.0071, -0.0044]) \n업데이트결과: tensor([2.4146, 4.0232])\nloss = 26.48 \n업데이트폭 = tensor([ 0.0058, -0.0035]) \n업데이트결과: tensor([2.4204, 4.0197])\nloss = 26.44 \n업데이트폭 = tensor([ 0.0047, -0.0029]) \n업데이트결과: tensor([2.4251, 4.0168])\nloss = 26.41 \n업데이트폭 = tensor([ 0.0038, -0.0023]) \n업데이트결과: tensor([2.4290, 4.0144])\n\n\n\n\nB. yhat의 관점에서 시각화"
  }
]